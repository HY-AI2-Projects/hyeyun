# DETR:End-to-End-Object-Detection-with-Transformers

â— 2023-2 ì¸ê³µì§€ëŠ¥2 ê¸°ë§ê³¼ì œ(ì •í˜œìœ¤ 2020003945)

â— ë³¸ ê²Œì‹œë¬¼ì€ DETR ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ, ë”¥ëŸ¬ë‹ ê¸°ì´ˆì§€ì‹ì´ ìˆëŠ” ì´ˆë³´ìë“¤ì´ DETR ëª¨ë¸ì— ëŒ€í•´ ë³´ë‹¤ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±í•œ ë¬¸ì„œì…ë‹ˆë‹¤.

â— ë…¼ë¬¸ ì›ë³¸ê³¼ ì˜ˆì‹œ ì½”ë“œë¥¼ ì¶”ê°€ ìë£Œë¡œ ì²¨ë¶€í•˜ì˜€ìŠµë‹ˆë‹¤.


## ğŸ·ï¸ Introduction

DETR(End-to-End Object Detection with Transformers)ì€ Facebook Research íŒ€ì´ 2020ë…„ 8ì›”ì— ì»´í“¨í„° ë¹„ì „ í•™íšŒì¸ ECCVì—ì„œ ë°œí‘œí•œ ë…¼ë¬¸ì…ë‹ˆë‹¤. DETRì€ Transformer êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬, end-to-endë¡œ object detectionì„ ìˆ˜í–‰í•˜ë©´ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ê¸°ì¡´ object detection ë°©ë²•ë¡ ì€ prior knowledgeê°€ ë§ì´ ìš”êµ¬ë˜ì—ˆê³ , NMS(Non Maximum Suppression)ê³¼ ê°™ì€ post-processing ê³¼ì •ì´ ë°˜ë“œì‹œ í•„ìš”í–ˆìŠµë‹ˆë‹¤. ë°˜ë©´ DETRì€ object detectionì„ ì§ì ‘ set prediction ë¬¸ì œë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤. í˜„ì¬ ë§ì€ SOTA ëª¨ë¸ë“¤ì´ DETRì„ ê¸°ë°˜ìœ¼ë¡œ ë°œì „í•œë§Œí¼, ë°˜ë“œì‹œ ì½ì–´ì•¼í•˜ëŠ” ê¸°ë…ë¹„ì ì¸ ë…¼ë¬¸ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 



## ğŸ·ï¸ Contribution 

ë³¸ ë…¼ë¬¸ì—ì„œ ì£¼ì¥í•˜ëŠ” contributionì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” object detectionì„ direct set predictionìœ¼ë¡œ ì •ì˜í•˜ì—¬, transformerì™€ bipartite matching loss(ì´ë¶„ë§¤ì¹­)ë¥¼ ì‚¬ìš©í•œ DETR(DEtection TRansformer)ì„ ì œì•ˆí•©ë‹ˆë‹¤. 
2. DETRì€ COCO datasetì— ëŒ€í•˜ì—¬ Faster R-CNNê³¼ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. 
3. ì¶”ê°€ì ìœ¼ë¡œ, self-attentionì„ í†µí•œ global information(ì „ì—­ ì •ë³´)ë¥¼ í™œìš©í•¨ìœ¼ë¡œì¨ í¬ê¸°ê°€ í° ê°ì²´ë¥¼ Faster R-CNNë³´ë‹¤ í›¨ì”¬ ì˜ í¬ì°©í•©ë‹ˆë‹¤.



## ğŸ·ï¸ Method 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” object detection ì‹œ direct set predictionì„ ìœ„í•´ ë‘ ê°€ì§€ ìš”ì†Œê°€ í•„ìˆ˜ì ì´ë¼ê³  í•©ë‹ˆë‹¤. 

(1) predicted bounding boxì™€ ground truth box ì‚¬ì´ì˜ unique matchingì„ ê°€ëŠ¥í•˜ë„ë¡ í•˜ëŠ” set prediction loss

(2) í•œ ë²ˆì˜ forward passë¡œ object model ì‚¬ì´ì˜ relationì„ ì˜ˆì¸¡í•˜ëŠ” architecture


1. Object detection set prediction loss
   
![image](https://github.com/hyeyun0302/DETR_End-to-End-Object-Detection-with-Transformers/assets/104217871/d0e32cc8-52f6-42a4-a13d-43e73d608e2b)

ë¨¼ì € ì²« ë²ˆì§¸ ì¡°ê±´ (1)ì„ ì¶©ì¡±í•˜ê¸° ìœ„í•´ lossë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì€ ë‘ ë‹¨ê³„ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤. ì²« ë²ˆì§¸ë¡œ, predicted bounding boxì™€ ground truth box ì‚¬ì´ì˜ uniqueí•œ matchingì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” matchingëœ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ hungarian lossë¥¼ ì—°ì‚°í•©ë‹ˆë‹¤. ì´ ì¤‘ ë¨¼ì € ì²« ë²ˆì§¸ ë‹¨ê³„ë¶€í„° ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. 

1.1 Find optimal matching 

ê¸°ì¡´ì˜ ì—°êµ¬ëŠ” ìˆ˜ ì²œê°œì˜ anchorë¥¼ ìƒì„±í•˜ì—¬, ê°ì²´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ proposalë¡œ ì‚¬ìš©í•˜ëŠ”ë° ì´ëŠ” ê°ì²´ê°€ â€œì–¼ë§ˆë‚˜â€ ìˆëŠ”ì§€ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ DETRì€ ê³ ì •ëœ í¬ê¸°ì˜ 
N
ê°œì˜ predictionë§Œì„ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨, ìˆ˜ë§ì€ anchorë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ìš°íšŒí•©ë‹ˆë‹¤. ì´ ë•Œ 
N
ì€ ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ ë‚´ ì¡´ì¬í•˜ëŠ” ê°ì²´ì˜ ìˆ˜ë³´ë‹¤ í›¨ì”¬ ë” í° ìˆ˜ë¡œ ì§€ì •í–ˆìŠµë‹ˆë‹¤. ì¦‰, ì´ëŠ” DETRì„ í†µí•´ ì˜ˆì¸¡í•˜ëŠ” ê°ì²´ì˜ ìˆ˜ëŠ” ìµœëŒ€ 
N
ê°œì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì ì€ ìˆ˜ì˜ predictionì´ ìƒì„±ë˜ì–´, ground truthì™€ì˜ unique matchingì„ ìƒëŒ€ì ìœ¼ë¡œ ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.




ì´ëŠ” ì´ë¶„ ë§¤ì¹­ì„ í†µí•´ ê³ ìœ í•œ ì˜ˆì¸¡ì„ ê°•ì œí•˜ëŠ” ì§‘í•© ê¸°ë°˜ ì „ì—­ ì†ì‹¤ê³¼ Transformer encoder-decoder ì•„í‚¤í…ì²˜ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. í•™ìŠµëœ ê°ì²´ ì¿¼ë¦¬ì˜ ê³ ì •ëœ ì‘ì€ ì§‘í•©ì´ ì£¼ì–´ì§€ë©´ ê°ì²´ì˜ ê´€ê³„ì™€ ê¸€ë¡œë²Œ ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ê°€ ìµœì¢… ì˜ˆì¸¡ ì§‘í•©ì„ ì§ì ‘ ë³‘ë ¬ë¡œ ì¶œë ¥í•´ì•¼ í•˜ëŠ” ì´ìœ ê°€ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³‘ë ¬ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ DETRì€ ë§¤ìš° ë¹ ë¥´ê³  íš¨ìœ¨ì ì…ë‹ˆë‹¤.

ê°ì²´ ê²€ì¶œì´ ë¶„ë¥˜ë³´ë‹¤ ì–´ë µì§€ ì•Šê³ , í›ˆë ¨ê³¼ ì¶”ë¡ ì„ ìœ„í•´ ë³µì¡í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í•„ìš”ë¡œ í•´ì„œëŠ” ì•ˆ ëœë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. DETRì€ êµ¬í˜„ê³¼ ì‹¤í—˜ì´ ë§¤ìš° ê°„ë‹¨í•˜ë©°, ì €í¬ëŠ” DETRë¡œ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì„ PyTorch ì½”ë“œì˜ ëª‡ ê°€ì§€ ë¼ì¸ì—ì„œë§Œ ë³´ì—¬ì£¼ëŠ” ë…ë¦½í˜• Colab Notebookì„ ì œê³µí•©ë‹ˆë‹¤. í›ˆë ¨ ì½”ë“œëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì•„ë‹ˆë¼ í‘œì¤€ í›ˆë ¨ ë£¨í”„ë¥¼ ê°€ì§„ main.py ê°€ì ¸ì˜¤ê¸° ëª¨ë¸ ë° ê¸°ì¤€ ì •ì˜ë¼ëŠ” ì•„ì´ë””ì–´ë¥¼ ë”°ë¦…ë‹ˆë‹¤.


ì €í¬ëŠ” ê¸°ë³¸ DETR ë° DETR-DC5 ëª¨ë¸ì„ ì œê³µí•˜ë©°, í–¥í›„ ë” ë§ì€ ëª¨ë¸ì„ í¬í•¨í•  ê³„íšì…ë‹ˆë‹¤. APëŠ” COCO 2017 val5kì—ì„œ ê³„ì‚°ë˜ë©°, í† ì¹˜ìŠ¤í¬ë¦½íŠ¸ ë³€í™˜ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ìŒ 100 val5k COCO ì´ë¯¸ì§€ ì´ìƒì˜ ì¶”ë¡  ì‹œê°„ì„ ê°–ìŠµë‹ˆë‹¤.


	name	backbone	schedule	inf_time	box AP	url	size
0	DETR	R50	500	0.036	42.0	model | logs	159Mb
1	DETR-DC5	R50	500	0.083	43.3	model | logs	159Mb
2	DETR	R101	500	0.050	43.5	model | logs	232Mb
3	DETR-DC5	R101	500	0.097	44.9	model | logs	232Mb


COCO val5k í‰ê°€ ê²°ê³¼ëŠ” ì´ ìš”ì§€ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ëª¨ë¸ì€ í† ì¹˜ í—ˆë¸Œë¥¼ í†µí•´ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ì „ êµìœ¡ëœ ì¤‘ëŸ‰ìœ¼ë¡œ DERTR50ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

model = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)

COCO panoptic val5k models:

	name	backbone	box AP	segm AP	PQ	url	size
0	DETR	R50	38.8	31.1	43.4	download	165Mb
1	DETR-DC5	R50	40.2	31.9	44.6	download	165Mb
2	DETR	R101	40.1	33	45.1	download	237Mb


##Notebooks

DETRì— ëŒ€í•œ ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ë…¸íŠ¸ë¶ì„ ì½œë¼ë¸Œë¡œ ì œê³µí•©ë‹ˆë‹¤:

DETRì˜ Colab ë…¸íŠ¸ë¶ í™œìš©: í—ˆë¸Œì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì˜ˆì¸¡ì„ ìƒì„±í•œ ë‹¤ìŒ ëª¨ë¸ì˜ ì£¼ì˜ë ¥ì„ ì‹œê°í™”í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤(ë…¼ë¬¸ì˜ ê·¸ë¦¼ê³¼ ìœ ì‚¬)
ë…ë¦½í˜• ì½œë© ë…¸íŠ¸ë¶: ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” DETRì˜ ë‹¨ìˆœí™”ëœ ë²„ì „ì„ 50ì¤„ì˜ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•œ ë‹¤ìŒ ì˜ˆì¸¡ì„ ì‹œê°í™”í•˜ëŠ” ë°©ë²•ì„ ì‹œì—°í•©ë‹ˆë‹¤. ì½”ë“œë² ì´ìŠ¤ì— ë“¤ì–´ê°€ê¸° ì „ì— ì•„í‚¤í…ì²˜ë¥¼ ë” ì˜ ì´í•´í•˜ê³  ì£¼ë³€ì„ ëŒì•„ë‹¤ë…€ì•¼ í•œë‹¤ë©´ ì¢‹ì€ ì¶œë°œì ì´ ë  ê²ƒì…ë‹ˆë‹¤.
Panoptic Colab ë…¸íŠ¸ë¶: Panoptic ë¶„í•  ë° ploë¥¼ ìœ„í•´ DETRì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì‹œì—°í•©ë‹ˆë‹¤t the predictions.

##Usage - Object detection
DRIGì—ëŠ” ì»´íŒŒì¼ëœ ì¶”ê°€ êµ¬ì„± ìš”ì†Œê°€ ì—†ê³  íŒ¨í‚¤ì§€ ì¢…ì†ì„±ì´ ìµœì†Œì´ë¯€ë¡œ ì½”ë“œ ì‚¬ìš©ì´ ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤. ì €í¬ëŠ” ì½˜ë‹¤ë¥¼ í†µí•´ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. ë¨¼ì € ì €ì¥ì†Œë¥¼ ë¡œì»¬ë¡œ ë³µì œí•©ë‹ˆë‹¤:

git clone https://github.com/facebookresearch/detr.git
install PyTorch 1.5+ and torchvision 0.6+:

conda install -c pytorch pytorch torchvision
Install pycocotools (for evaluation on COCO) and scipy (for training):
conda install cython scipy
pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'

detection ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.

(optional) to work with panoptic install panopticapi:
pip install git+https://github.com/cocodataset/panopticapi.git


<div align="center">
  <img src="./assets/logo_2.png" width="30%">
</div>
<h2 align="center">ğŸ¦–detrex: Benchmarking Detection Transformers</h2>
<p align="center">
    <a href="https://github.com/IDEA-Research/detrex/releases">
        <img alt="release" src="https://img.shields.io/github/v/release/IDEA-Research/detrex">
    </a>
    <a href="https://detrex.readthedocs.io/en/latest/index.html">
        <img alt="docs" src="https://img.shields.io/badge/docs-latest-blue">
    </a>
    <a href='https://detrex.readthedocs.io/en/latest/?badge=latest'>
    <img src='https://readthedocs.org/projects/detrex/badge/?version=latest' alt='Documentation Status' />
    </a>
    <a href="https://github.com/IDEA-Research/detrex/blob/main/LICENSE">
        <img alt="GitHub" src="https://img.shields.io/github/license/IDEA-Research/detrex.svg?color=blue">
    </a>
    <a href="https://github.com/IDEA-Research/detrex/pulls">
        <img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-welcome-pink.svg">
    </a>
    <a href="https://github.com/IDEA-Research/detrex/issues">
        <img alt="open issues" src="https://img.shields.io/github/issues/IDEA-Research/detrex">
    </a>
</p>


<div align="center">

<!-- <a href="https://arxiv.org/abs/2306.07265">ğŸ“šRead detrex Benchmarking Paper</a> <sup><i><font size="3" color="#FF0000">New</font></i></sup> |
<a href="https://rentainhe.github.io/projects/detrex/">ğŸ Project Page</a> <sup><i><font size="3" color="#FF0000">New</font></i></sup> |  [ğŸ·ï¸Cite detrex](#citation) -->

[ğŸ“šRead detrex Benchmarking Paper](https://arxiv.org/abs/2306.07265) | [ğŸ Project Page](https://rentainhe.github.io/projects/detrex/) | [ğŸ·ï¸Cite detrex](#citation) | [ğŸš¢DeepDataSpace](https://github.com/IDEA-Research/deepdataspace)

</div>


<div align="center">

[ğŸ“˜Documentation](https://detrex.readthedocs.io/en/latest/index.html) |
[ğŸ› ï¸Installation](https://detrex.readthedocs.io/en/latest/tutorials/Installation.html) |
[ğŸ‘€Model Zoo](https://detrex.readthedocs.io/en/latest/tutorials/Model_Zoo.html) |
[ğŸš€Awesome DETR](https://github.com/IDEA-Research/awesome-detection-transformer) |
[ğŸ†•News](#whats-new) |
[ğŸ¤”Reporting Issues](https://github.com/IDEA-Research/detrex/issues/new/choose)

</div>


## Introduction

detrex is an open-source toolbox that provides state-of-the-art Transformer-based detection algorithms. It is built on top of [Detectron2](https://github.com/facebookresearch/detectron2) and its module design is partially borrowed from [MMDetection](https://github.com/open-mmlab/mmdetection) and [DETR](https://github.com/facebookresearch/detr). Many thanks for their nicely organized code. The main branch works with **Pytorch 1.10+** or higher (we recommend **Pytorch 1.12**).

<div align="center">
  <img src="./assets/detr_arch.png" width="100%"/>
</div>

<details open>
<summary> Major Features </summary>

- **Modular Design.** detrex decomposes the Transformer-based detection framework into various components which help users easily build their own customized models.

- **Strong Baselines.** detrex provides a series of strong baselines for Transformer-based detection models. We have further boosted the model performance from **0.2 AP** to **1.1 AP** through optimizing hyper-parameters among most of the supported algorithms.

- **Easy to Use.** detrex is designed to be **light-weight** and easy for users to use:
  - [LazyConfig System](https://detectron2.readthedocs.io/en/latest/tutorials/lazyconfigs.html) for more flexible syntax and cleaner config files.
  - Light-weight [training engine](./tools/train_net.py) modified from detectron2 [lazyconfig_train_net.py](https://github.com/facebookresearch/detectron2/blob/main/tools/lazyconfig_train_net.py)

Apart from detrex, we also released a repo [Awesome Detection Transformer](https://github.com/IDEA-Research/awesome-detection-transformer) to present papers about Transformer for detection and segmentation.

</details>

## Fun Facts
The repo name detrex has several interpretations:
- <font color=blue> <b> detr-ex </b> </font>: We take our hats off to DETR and regard this repo as an extension of Transformer-based detection algorithms.

- <font color=#db7093> <b> det-rex </b> </font>: rex literally means 'king' in Latin. We hope this repo can help advance the state of the art on object detection by providing the best Transformer-based detection algorithms from the research community.

- <font color=#008000> <b> de-t.rex </b> </font>: de means 'the' in Dutch. T.rex, also called Tyrannosaurus Rex, means 'king of the tyrant lizards' and connects to our research work 'DINO', which is short for Dinosaur.

## What's New
v0.5.0 was released on 16/07/2023:
- Support [Focus-DETR (ICCV'2023)](./projects/focus_detr/).
- Support [SQR-DETR (CVPR'2023)](https://github.com/IDEA-Research/detrex/tree/main/projects/sqr_detr), credits to [Fangyi Chen](https://github.com/Fangyi-Chen)
- Support [Align-DETR (ArXiv'2023)](./projects/align_detr/), credits to [Zhi Cai](https://github.com/FelixCaae)
- Support [EVA-01 (CVPR'2023 Highlight)](https://github.com/baaivision/EVA/tree/master/EVA-01) and [EVA-02 (ArXiv'2023)](https://github.com/baaivision/EVA/tree/master/EVA-02) backbones, please check [DINO-EVA](./projects/dino_eva/) for more benchmarking results.

Please see [changelog.md](./changlog.md) for details and release history.

## Installation

Please refer to [Installation Instructions](https://detrex.readthedocs.io/en/latest/tutorials/Installation.html) for the details of installation.

## Getting Started

Please refer to [Getting Started with detrex](https://detrex.readthedocs.io/en/latest/tutorials/Getting_Started.html) for the basic usage of detrex. We also provides other tutorials for:
- [Learn about the config system of detrex](https://detrex.readthedocs.io/en/latest/tutorials/Config_System.html)
- [How to convert the pretrained weights from original detr repo into detrex format](https://detrex.readthedocs.io/en/latest/tutorials/Converters.html)
- [Visualize your training data and testing results on COCO dataset](https://detrex.readthedocs.io/en/latest/tutorials/Tools.html#visualization)
- [Analyze the model under detrex](https://detrex.readthedocs.io/en/latest/tutorials/Tools.html#model-analysis)
- [Download and initialize with the pretrained backbone weights](https://detrex.readthedocs.io/en/latest/tutorials/Using_Pretrained_Backbone.html)
- [Frequently asked questions](https://github.com/IDEA-Research/detrex/issues/109)
- [A simple onnx convert tutorial provided by powermano](https://github.com/IDEA-Research/detrex/issues/192)
- Simple training techniques: [Model-EMA](https://github.com/IDEA-Research/detrex/pull/201), [Mixed Precision Training](https://github.com/IDEA-Research/detrex/pull/198), [Activation Checkpoint](https://github.com/IDEA-Research/detrex/pull/200)
- [Simple tutorial about custom dataset training](https://github.com/IDEA-Research/detrex/pull/187)

Although some of the tutorials are currently presented with relatively simple content, we will constantly improve our documentation to help users achieve a better user experience.

## Documentation

Please see [documentation](https://detrex.readthedocs.io/en/latest/index.html) for full API documentation and tutorials.

## Model Zoo
Results and models are available in [model zoo](https://detrex.readthedocs.io/en/latest/tutorials/Model_Zoo.html).

<details open>
<summary> Supported methods </summary>

- [x] [DETR (ECCV'2020)](./projects/detr/)
- [x] [Deformable-DETR (ICLR'2021 Oral)](./projects/deformable_detr/)
- [x] [PnP-DETR (ICCV'2021)](./projects/pnp_detr/)
- [x] [Conditional-DETR (ICCV'2021)](./projects/conditional_detr/)
- [x] [Anchor-DETR (AAAI 2022)](./projects/anchor_detr/)
- [x] [DAB-DETR (ICLR'2022)](./projects/dab_detr/)
- [x] [DAB-Deformable-DETR (ICLR'2022)](./projects/dab_deformable_detr/)
- [x] [DN-DETR (CVPR'2022 Oral)](./projects/dn_detr/)
- [x] [DN-Deformable-DETR (CVPR'2022 Oral)](./projects/dn_deformable_detr/)
- [x] [Group-DETR (ICCV'2023)](./projects/group_detr/)
- [x] [DETA (ArXiv'2022)](./projects/deta/)
- [x] [DINO (ICLR'2023)](./projects/dino/)
- [x] [H-Deformable-DETR (CVPR'2023)](./projects/h_deformable_detr/)
- [x] [MaskDINO (CVPR'2023)](./projects/maskdino/)
- [x] [CO-MOT (ArXiv'2023)](./projects/co_mot/)
- [x] [SQR-DETR (CVPR'2023)](./projects/sqr_detr/)
- [x] [Align-DETR (ArXiv'2023)](./projects/align_detr/)
- [x] [EVA-01 (CVPR'2023 Highlight)](./projects/dino_eva/)
- [x] [EVA-02 (ArXiv'2023)](./projects/dino_eva/)
- [x] [Focus-DETR (ICCV'2023)](./projects/focus_detr/)

Please see [projects](./projects/) for the details about projects that are built based on detrex.

</details>


## License

This project is released under the [Apache 2.0 license](LICENSE).


## Acknowledgement
- detrex is an open-source toolbox for Transformer-based detection algorithms created by researchers of **IDEACVR**. We appreciate all contributions to detrex!
- detrex is built based on [Detectron2](https://github.com/facebookresearch/detectron2) and part of its module design is borrowed from [MMDetection](https://github.com/open-mmlab/mmdetection), [DETR](https://github.com/facebookresearch/detr), and [Deformable-DETR](https://github.com/fundamentalvision/Deformable-DETR).


## Citation
If you use this toolbox in your research or wish to refer to the baseline results published here, please use the following BibTeX entries:

- Citing **detrex**:

```BibTeX
@misc{ren2023detrex,
      title={detrex: Benchmarking Detection Transformers}, 
      author={Tianhe Ren and Shilong Liu and Feng Li and Hao Zhang and Ailing Zeng and Jie Yang and Xingyu Liao and Ding Jia and Hongyang Li and He Cao and Jianan Wang and Zhaoyang Zeng and Xianbiao Qi and Yuhui Yuan and Jianwei Yang and Lei Zhang},
      year={2023},
      eprint={2306.07265},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

<details>
<summary> Citing Supported Algorithms </summary>

```BibTex
@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@inproceedings{
  zhu2021deformable,
  title={Deformable {\{}DETR{\}}: Deformable Transformers for End-to-End Object Detection},
  author={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=gZ9hCDWe6ke}
}

@inproceedings{meng2021-CondDETR,
  title       = {Conditional DETR for Fast Training Convergence},
  author      = {Meng, Depu and Chen, Xiaokang and Fan, Zejia and Zeng, Gang and Li, Houqiang and Yuan, Yuhui and Sun, Lei and Wang, Jingdong},
  booktitle   = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  year        = {2021}
}

@inproceedings{
  liu2022dabdetr,
  title={{DAB}-{DETR}: Dynamic Anchor Boxes are Better Queries for {DETR}},
  author={Shilong Liu and Feng Li and Hao Zhang and Xiao Yang and Xianbiao Qi and Hang Su and Jun Zhu and Lei Zhang},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=oMI9PjOb9Jl}
}

@inproceedings{li2022dn,
  title={Dn-detr: Accelerate detr training by introducing query denoising},
  author={Li, Feng and Zhang, Hao and Liu, Shilong and Guo, Jian and Ni, Lionel M and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13619--13627},
  year={2022}
}

@inproceedings{
  zhang2023dino,
  title={{DINO}: {DETR} with Improved DeNoising Anchor Boxes for End-to-End Object Detection},
  author={Hao Zhang and Feng Li and Shilong Liu and Lei Zhang and Hang Su and Jun Zhu and Lionel Ni and Heung-Yeung Shum},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=3mRwyG5one}
}

@InProceedings{Chen_2023_ICCV,
  author    = {Chen, Qiang and Chen, Xiaokang and Wang, Jian and Zhang, Shan and Yao, Kun and Feng, Haocheng and Han, Junyu and Ding, Errui and Zeng, Gang and Wang, Jingdong},
  title     = {Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2023},
  pages     = {6633-6642}
}

@InProceedings{Jia_2023_CVPR,
  author    = {Jia, Ding and Yuan, Yuhui and He, Haodi and Wu, Xiaopei and Yu, Haojun and Lin, Weihong and Sun, Lei and Zhang, Chao and Hu, Han},
  title     = {DETRs With Hybrid Matching},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2023},
  pages     = {19702-19712}
}

@InProceedings{Li_2023_CVPR,
  author    = {Li, Feng and Zhang, Hao and Xu, Huaizhe and Liu, Shilong and Zhang, Lei and Ni, Lionel M. and Shum, Heung-Yeung},
  title     = {Mask DINO: Towards a Unified Transformer-Based Framework for Object Detection and Segmentation},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2023},
  pages     = {3041-3050}
}

@article{yan2023bridging,
  title={Bridging the Gap Between End-to-end and Non-End-to-end Multi-Object Tracking},
  author={Yan, Feng and Luo, Weixin and Zhong, Yujie and Gan, Yiyang and Ma, Lin},
  journal={arXiv preprint arXiv:2305.12724},
  year={2023}
}

@InProceedings{Chen_2023_CVPR,
  author    = {Chen, Fangyi and Zhang, Han and Hu, Kai and Huang, Yu-Kai and Zhu, Chenchen and Savvides, Marios},
  title     = {Enhanced Training of Query-Based Object Detection via Selective Query Recollection},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2023},
  pages     = {23756-23765}
}
```


</details>



